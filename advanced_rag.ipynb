{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xu9wsiutectG",
      "metadata": {
        "collapsed": true,
        "id": "Xu9wsiutectG"
      },
      "outputs": [],
      "source": [
        "!pip install pymilvus pymilvus[milvus_lite] datasets transformers sentence-transformers ragas evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fc17bf",
      "metadata": {
        "id": "49fc17bf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers, torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
        "\n",
        "from evaluate import load\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "from ragas.run_config import RunConfig\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275ab245",
      "metadata": {
        "id": "275ab245"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0e11a5",
      "metadata": {
        "id": "ff0e11a5"
      },
      "outputs": [],
      "source": [
        "passages = pd.read_parquet(\n",
        "    \"hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet\"\n",
        ")\n",
        "\n",
        "print(passages.shape)\n",
        "passages.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0pYcegiE9mc",
      "metadata": {
        "id": "Q0pYcegiE9mc"
      },
      "outputs": [],
      "source": [
        "queries = pd.read_parquet(\n",
        "    \"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\"\n",
        ")\n",
        "\n",
        "print(queries.shape)\n",
        "queries.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45249cd2",
      "metadata": {
        "id": "45249cd2"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0d0a82",
      "metadata": {
        "id": "aa0d0a82"
      },
      "outputs": [],
      "source": [
        "# Analyze passage lengths\n",
        "passages[\"length\"] = passages[\"passage\"].str.len()\n",
        "print(f\"Min length: {passages['length'].min()}\")\n",
        "print(f\"Max length: {passages['length'].max()}\")\n",
        "print(f\"Mean length: {passages['length'].mean():.2f}\")\n",
        "print(f\"Median length: {passages['length'].median()}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values: {passages['passage'].isna().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zIC9odDvAmob",
      "metadata": {
        "id": "zIC9odDvAmob"
      },
      "source": [
        "# Setup Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W3XPrTbbAwGr",
      "metadata": {
        "id": "W3XPrTbbAwGr"
      },
      "source": [
        "## Prompts\n",
        "Using Basic Prompt as it performed the best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t7-xoSTZAvTX",
      "metadata": {
        "id": "t7-xoSTZAvTX"
      },
      "outputs": [],
      "source": [
        "def generate_basic_prompt(query, context):\n",
        "    return f\"Context: {context}: \\n Question: {query} \""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Krvx60BHAyDd",
      "metadata": {
        "id": "Krvx60BHAyDd"
      },
      "source": [
        "## Embedding Models\n",
        "Using MiniLM-L6-v2 as it had better performance than mpnet-base-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UjH5_GVGA4M3",
      "metadata": {
        "id": "UjH5_GVGA4M3"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xvk3HnGNOKGz",
      "metadata": {
        "id": "xvk3HnGNOKGz"
      },
      "source": [
        "## Top k\n",
        "\n",
        "*   Retrieving 10: We retrieve 10 candidates to maximize recall, since relevant passages often appear beyond the top-5 (e.g., recall is higher at k=10 vs k=3/5)\n",
        "*   Using top-5: We then feed only the top-5 (after reranking) to the model to boost precision and faithfulness, avoiding the noise and dilution that shows up when all 10 are passed in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pOZYmwcSON-Y",
      "metadata": {
        "id": "pOZYmwcSON-Y"
      },
      "outputs": [],
      "source": [
        "retrieval_top_k = 10\n",
        "context_top_k = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2zsnOQVNKwaB",
      "metadata": {
        "id": "2zsnOQVNKwaB"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Ap1GCf9Kxsy",
      "metadata": {
        "id": "1Ap1GCf9Kxsy"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ebWE7ADSPx",
      "metadata": {
        "id": "66ebWE7ADSPx"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vd6Pe74ZDPd7",
      "metadata": {
        "id": "vd6Pe74ZDPd7"
      },
      "outputs": [],
      "source": [
        "def create_embeddings_and_rag_data(model):\n",
        "    embeddings = model.encode(\n",
        "        passages[\"passage\"].tolist(),\n",
        "        convert_to_tensor=True,\n",
        "        show_progress_bar=False,\n",
        "        batch_size=64,\n",
        "    )\n",
        "\n",
        "    rag_data = [\n",
        "        {\n",
        "            \"id\": idx,\n",
        "            \"passage\": passages.iloc[idx][\"passage\"],\n",
        "            \"embedding\": embeddings[idx].tolist(),\n",
        "        }\n",
        "        for idx in range(len(passages))\n",
        "    ]\n",
        "\n",
        "    return embeddings, rag_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-fzauGn_Hcys",
      "metadata": {
        "id": "-fzauGn_Hcys"
      },
      "outputs": [],
      "source": [
        "def create_schema(embed_dim):\n",
        "    id_ = FieldSchema(\n",
        "        name=\"id\",\n",
        "        dtype=DataType.INT64,\n",
        "        is_primary=True,\n",
        "        auto_id=False,\n",
        "    )\n",
        "\n",
        "    passage = FieldSchema(\n",
        "        name=\"passage\",\n",
        "        dtype=DataType.VARCHAR,\n",
        "        max_length=2600,\n",
        "    )\n",
        "    embedding = FieldSchema(\n",
        "        name=\"embedding\",\n",
        "        dtype=DataType.FLOAT_VECTOR,\n",
        "        dim=embed_dim,\n",
        "    )\n",
        "\n",
        "    schema = CollectionSchema(\n",
        "        fields=[id_, passage, embedding],\n",
        "        description=\"RAG Wikipedia passages\",\n",
        "        auto_id=False,\n",
        "    )\n",
        "\n",
        "    return schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j0B_-nCeHJA9",
      "metadata": {
        "id": "j0B_-nCeHJA9"
      },
      "outputs": [],
      "source": [
        "def setup_collection(embed_dim, collection_name, rag_data):\n",
        "    if collection_name in client.list_collections():\n",
        "        client.drop_collection(collection_name)\n",
        "\n",
        "    schema = create_schema(embed_dim)\n",
        "\n",
        "    client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        schema=schema,\n",
        "    )\n",
        "\n",
        "    client.insert(collection_name=collection_name, data=rag_data)\n",
        "\n",
        "    index_params = MilvusClient.prepare_index_params()\n",
        "    index_params.add_index(\n",
        "        field_name=\"embedding\",\n",
        "        index_type=\"FLAT\",\n",
        "        metric_type=\"COSINE\",\n",
        "    )\n",
        "    client.create_index(\n",
        "        collection_name=collection_name,\n",
        "        index_params=index_params,\n",
        "    )\n",
        "\n",
        "    client.load_collection(collection_name=collection_name)\n",
        "\n",
        "    print(f\"{collection_name} created and loaded into memory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-eXXuxtqDPWE",
      "metadata": {
        "id": "-eXXuxtqDPWE"
      },
      "outputs": [],
      "source": [
        "def generate_answer(prompt, model, tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return answer[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ocyt0Etl-hdp",
      "metadata": {
        "id": "Ocyt0Etl-hdp"
      },
      "outputs": [],
      "source": [
        "get_output_file_name = lambda exp_name: f\"out_{exp_name}.json\"\n",
        "get_results_file_name = lambda exp_name: f\"exp_{exp_name}.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6wnk6HQzG-",
      "metadata": {
        "id": "4c6wnk6HQzG-"
      },
      "outputs": [],
      "source": [
        "def save_outputs(results, exp_name):\n",
        "    filename = get_output_file_name(exp_name)\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    print(f\"Saved {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7_FP94-FDPTL",
      "metadata": {
        "id": "7_FP94-FDPTL"
      },
      "outputs": [],
      "source": [
        "def save_results(config, metrics, exp_name):\n",
        "    output = {\n",
        "        \"config\": config,\n",
        "        \"metrics\": metrics,\n",
        "    }\n",
        "\n",
        "    filename = get_results_file_name(exp_name)\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(output, f)\n",
        "\n",
        "    print(f\"Saved {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ndtpi_-Ps4yy",
      "metadata": {
        "id": "Ndtpi_-Ps4yy"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def select_random_subset(results, size=25):\n",
        "    random_indices = np.random.choice(len(results), size=size, replace=False)\n",
        "    return [results[i] for i in random_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJPC6mHvRLtW",
      "metadata": {
        "collapsed": true,
        "id": "EJPC6mHvRLtW"
      },
      "outputs": [],
      "source": [
        "squad_metric = load(\"squad\")\n",
        "\n",
        "\n",
        "def perform_basic_evaluation(results):\n",
        "    predictions = [\n",
        "        {\n",
        "            \"id\": str(i),\n",
        "            \"prediction_text\": r[\"predicted_answer\"],\n",
        "        }\n",
        "        for i, r in enumerate(results)\n",
        "    ]\n",
        "\n",
        "    references = [\n",
        "        {\n",
        "            \"id\": str(i),\n",
        "            \"answers\": {\n",
        "                \"text\": [r[\"ground_truth\"]],\n",
        "                \"answer_start\": [0],\n",
        "            },\n",
        "        }\n",
        "        for i, r in enumerate(results)\n",
        "    ]\n",
        "\n",
        "    metrics = squad_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {\n",
        "        \"f1_score\": metrics[\"f1\"],\n",
        "        \"exact_match\": metrics[\"exact_match\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "Q093f-ApSTNL",
      "metadata": {
        "id": "Q093f-ApSTNL"
      },
      "outputs": [],
      "source": [
        "def perform_ragas_evaluation(results):\n",
        "    results = select_random_subset(results, size=100)\n",
        "\n",
        "    data = {\n",
        "        \"question\": [r[\"question\"] for r in results],\n",
        "        \"answer\": [r[\"predicted_answer\"] for r in results],\n",
        "        \"contexts\": [r[\"contexts\"] for r in results],\n",
        "        \"ground_truth\": [r[\"ground_truth\"] for r in results],\n",
        "    }\n",
        "\n",
        "    dataset = Dataset.from_dict(data)\n",
        "\n",
        "    config = RunConfig(max_workers=8, timeout=60)\n",
        "    eval_result_sequential = evaluate(\n",
        "        dataset, metrics=[answer_relevancy], run_config=config\n",
        "    )\n",
        "\n",
        "    eval_result_parallel = evaluate(\n",
        "        dataset,\n",
        "        metrics=[faithfulness, context_recall, context_precision],\n",
        "        run_config=config,\n",
        "    )\n",
        "\n",
        "    # Combine results\n",
        "    agg_scores = {}\n",
        "\n",
        "    for metric, values in eval_result_parallel._scores_dict.items():\n",
        "        agg_scores[metric] = float(np.nanmean(values))\n",
        "\n",
        "    for metric, values in eval_result_sequential._scores_dict.items():\n",
        "        agg_scores[metric] = float(np.nanmean(values))\n",
        "\n",
        "    return agg_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OsTnRYE0Q-7A",
      "metadata": {
        "id": "OsTnRYE0Q-7A"
      },
      "outputs": [],
      "source": [
        "def evaluate_results(results):\n",
        "    return {**perform_basic_evaluation(results), **perform_ragas_evaluation(results)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oMIH5XhMWpSv",
      "metadata": {
        "id": "oMIH5XhMWpSv"
      },
      "source": [
        "# Helper Functions for Advanced RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vaq7Ixj-Wr4Q",
      "metadata": {
        "id": "Vaq7Ixj-Wr4Q"
      },
      "source": [
        "## Query Rewriting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "by5b2ejMWu2v",
      "metadata": {
        "id": "by5b2ejMWu2v"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "\n",
        "def rewrite_query(query):\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "        You are a query rewriting assistant.Rewrite the given question into a concise keyword-style query optimized for Wikipedia retrieval in a RAG setup.\n",
        "        Rules:\n",
        "        - Keep it as concise as possible.\n",
        "        - Remove filler words like 'what', 'is', 'was'.\n",
        "        - Do not explain.\n",
        "        - Output ONLY the rewritten query, nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-bsO74rtW9Gq",
      "metadata": {
        "id": "-bsO74rtW9Gq"
      },
      "source": [
        "## Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tZ0kK2YeXDYk",
      "metadata": {
        "id": "tZ0kK2YeXDYk"
      },
      "outputs": [],
      "source": [
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "\n",
        "def rerank(query, retrieved_docs, top_k=5):\n",
        "    pairs = [(query, doc[\"text\"]) for doc in retrieved_docs]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "\n",
        "    for i, score in enumerate(scores):\n",
        "        retrieved_docs[i][\"rerank_score\"] = float(score)\n",
        "\n",
        "    reranked = sorted(retrieved_docs, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "    return reranked[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjG9QFrEJJ2S",
      "metadata": {
        "id": "wjG9QFrEJJ2S"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JGyEcWV0DPMq",
      "metadata": {
        "id": "JGyEcWV0DPMq"
      },
      "outputs": [],
      "source": [
        "client = MilvusClient(\"rag_wikipedia_mini.db\")\n",
        "collection_name = \"all_MiniLM_L6_v2\"\n",
        "\n",
        "embeddings, rag_data = create_embeddings_and_rag_data(embedding_model)\n",
        "setup_collection(384, collection_name, rag_data)\n",
        "experiment_name = f\"advanced_all_MiniLM_L6_v2_basic_{retrieval_top_k}_{context_top_k}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MDm4SaHwROTX",
      "metadata": {
        "id": "MDm4SaHwROTX"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "if os.path.exists(get_output_file_name(experiment_name)):\n",
        "    print(f\"Output for {experiment_name} exists! Skipping...\")\n",
        "else:\n",
        "    for index, row in tqdm(queries.iterrows(), total=len(queries)):\n",
        "        query = row[\"question\"]\n",
        "\n",
        "        # Advanced RAG Optimization - Query Rewrite\n",
        "        query = rewrite_query(query)\n",
        "\n",
        "        search_results = client.search(\n",
        "            collection_name=collection_name,\n",
        "            data=[embedding_model.encode(query).tolist()],\n",
        "            output_fields=[\"passage\"],\n",
        "            limit=retrieval_top_k,\n",
        "        )\n",
        "\n",
        "        # Advanced RAG Optimization - Reranking\n",
        "        docs = [\n",
        "            {\"text\": hit.entity.get(\"passage\"), \"score\": hit.score}\n",
        "            for hit in search_results[0]\n",
        "        ]\n",
        "        top_results = rerank(query, docs, context_top_k)\n",
        "\n",
        "        context = [doc[\"text\"] for doc in top_results]\n",
        "        context_str = \"\\n\".join(context)\n",
        "\n",
        "        prompt = generate_basic_prompt(query, context_str)\n",
        "        answer = generate_answer(prompt, model, tokenizer)\n",
        "        results.append(\n",
        "            {\n",
        "                \"question\": row[\"question\"],\n",
        "                \"rewritten_query\": query,\n",
        "                \"predicted_answer\": answer,\n",
        "                \"ground_truth\": row[\"answer\"],\n",
        "                \"contexts\": context,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    save_outputs(results, experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AXxiUwY8RQve",
      "metadata": {
        "id": "AXxiUwY8RQve"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(get_results_file_name(experiment_name)):\n",
        "    print(f\"Results for {experiment_name} exists! Skipping...\")\n",
        "else:\n",
        "    if not results:\n",
        "        with open(get_output_file_name(experiment_name), \"r\") as f:\n",
        "            results = json.load(f)\n",
        "    metrics = evaluate_results(results)\n",
        "    save_results(\n",
        "        {\n",
        "            \"embedding_model\": \"all_MiniLM_L6_v2\",\n",
        "            \"prompt\": \"basic\",\n",
        "            \"top_k\": context_top_k,\n",
        "        },\n",
        "        metrics,\n",
        "        experiment_name,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
