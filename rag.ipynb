{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xu9wsiutectG",
      "metadata": {
        "collapsed": true,
        "id": "Xu9wsiutectG"
      },
      "outputs": [],
      "source": [
        "!pip install pymilvus pymilvus[milvus_lite] datasets transformers sentence-transformers ragas evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fc17bf",
      "metadata": {
        "id": "49fc17bf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers, torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
        "\n",
        "from evaluate import load\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "from ragas.run_config import RunConfig\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275ab245",
      "metadata": {
        "id": "275ab245"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0e11a5",
      "metadata": {
        "id": "ff0e11a5"
      },
      "outputs": [],
      "source": [
        "passages = pd.read_parquet(\n",
        "    \"hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet\"\n",
        ")\n",
        "\n",
        "print(passages.shape)\n",
        "passages.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0pYcegiE9mc",
      "metadata": {
        "id": "Q0pYcegiE9mc"
      },
      "outputs": [],
      "source": [
        "queries = pd.read_parquet(\n",
        "    \"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\"\n",
        ")\n",
        "\n",
        "print(queries.shape)\n",
        "queries.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45249cd2",
      "metadata": {
        "id": "45249cd2"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0d0a82",
      "metadata": {
        "id": "aa0d0a82"
      },
      "outputs": [],
      "source": [
        "# Analyze passage lengths\n",
        "passages[\"length\"] = passages[\"passage\"].str.len()\n",
        "print(f\"Min length: {passages['length'].min()}\")\n",
        "print(f\"Max length: {passages['length'].max()}\")\n",
        "print(f\"Mean length: {passages['length'].mean():.2f}\")\n",
        "print(f\"Median length: {passages['length'].median()}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values: {passages['passage'].isna().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zIC9odDvAmob",
      "metadata": {
        "id": "zIC9odDvAmob"
      },
      "source": [
        "# Setup Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W3XPrTbbAwGr",
      "metadata": {
        "id": "W3XPrTbbAwGr"
      },
      "source": [
        "## Prompts\n",
        "1. Basic Prompt (Only Context and Question)\n",
        "2. Persona Prompt\n",
        "3. CoT Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t7-xoSTZAvTX",
      "metadata": {
        "id": "t7-xoSTZAvTX"
      },
      "outputs": [],
      "source": [
        "def generate_basic_prompt(query, context):\n",
        "    return f\"Context: {context}: \\n Question: {query} \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l5QUKQQ2CWFD",
      "metadata": {
        "id": "l5QUKQQ2CWFD"
      },
      "outputs": [],
      "source": [
        "def generate_persona_prompt(query, context):\n",
        "    return f\"\"\"\n",
        "    You are a knowledgeable and trustworthy Wikipedia-style guide.\n",
        "    Your role is to explain answers clearly, objectively, and concisely, using only the retrieved passages.\n",
        "\n",
        "    Role alignment:\n",
        "    - Speak with the calm, factual tone of a reference editor.\n",
        "    - Present information as if you are curating reliable knowledge.\n",
        "    - Avoid speculation or personal opinions.\n",
        "\n",
        "    Guidelines:\n",
        "    - Keep answers short and direct, but add a brief explanation if it improves clarity.\n",
        "    - Use clear, well-structured sentences that feel authoritative and easy to read.\n",
        "    - If the passages do not contain the answer, say: \"The passage does not provide enough information.\"\n",
        "\n",
        "\n",
        "    Context: {context}: \\n Question: {query}\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cnldexs6CzWO",
      "metadata": {
        "id": "cnldexs6CzWO"
      },
      "outputs": [],
      "source": [
        "def generate_cot_prompt(query, context):\n",
        "    return f\"\"\"\n",
        "    You are an assistant that answers questions using only the retrieved passages.\n",
        "\n",
        "    Process:\n",
        "    1. Read the question carefully.\n",
        "    2. Identify the most relevant information in the passages.\n",
        "    3. Reason step by step to connect facts and resolve conflicts.\n",
        "    4. Give a clear and concise final answer.\n",
        "\n",
        "    Answer characteristics:\n",
        "    - Short and precise, avoiding unnecessary words.\n",
        "    - Faithful to the passages, with no outside knowledge.\n",
        "    - Direct phrasing that can be matched exactly when possible.\n",
        "    - If the passages do not provide enough information, reply: \"The passage does not provide enough information.\"\n",
        "\n",
        "    Context: {context}: \\n Question: {query}\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Krvx60BHAyDd",
      "metadata": {
        "id": "Krvx60BHAyDd"
      },
      "source": [
        "## Embedding Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UjH5_GVGA4M3",
      "metadata": {
        "id": "UjH5_GVGA4M3"
      },
      "outputs": [],
      "source": [
        "embedding_model_384 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embedding_model_768 = SentenceTransformer(\"all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2zsnOQVNKwaB",
      "metadata": {
        "id": "2zsnOQVNKwaB"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Ap1GCf9Kxsy",
      "metadata": {
        "id": "1Ap1GCf9Kxsy"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-PpEr8Tm_mE3",
      "metadata": {
        "id": "-PpEr8Tm_mE3"
      },
      "source": [
        "# Define the Experiment Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZpItj6BqAhbl",
      "metadata": {
        "id": "ZpItj6BqAhbl"
      },
      "outputs": [],
      "source": [
        "configs = {\n",
        "    \"prompts\": [\n",
        "        (\"basic\", generate_basic_prompt),\n",
        "        (\"persona\", generate_persona_prompt),\n",
        "        (\"cot\", generate_cot_prompt),\n",
        "    ],\n",
        "    \"embedding_models\": [\n",
        "        (\"all_MiniLM_L6_v2\", 384, embedding_model_384),\n",
        "        (\"all_mpnet_base_v2\", 768, embedding_model_768),\n",
        "    ],\n",
        "    \"top_k\": [3, 5, 10],\n",
        "}\n",
        "\n",
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ebWE7ADSPx",
      "metadata": {
        "id": "66ebWE7ADSPx"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vd6Pe74ZDPd7",
      "metadata": {
        "id": "vd6Pe74ZDPd7"
      },
      "outputs": [],
      "source": [
        "def create_embeddings_and_rag_data(model):\n",
        "    embeddings = model.encode(\n",
        "        passages[\"passage\"].tolist(),\n",
        "        convert_to_tensor=True,\n",
        "        show_progress_bar=False,\n",
        "        batch_size=64,\n",
        "    )\n",
        "\n",
        "    rag_data = [\n",
        "        {\n",
        "            \"id\": idx,\n",
        "            \"passage\": passages.iloc[idx][\"passage\"],\n",
        "            \"embedding\": embeddings[idx].tolist(),\n",
        "        }\n",
        "        for idx in range(len(passages))\n",
        "    ]\n",
        "\n",
        "    return embeddings, rag_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-fzauGn_Hcys",
      "metadata": {
        "id": "-fzauGn_Hcys"
      },
      "outputs": [],
      "source": [
        "def create_schema(embed_dim):\n",
        "    id_ = FieldSchema(\n",
        "        name=\"id\",\n",
        "        dtype=DataType.INT64,\n",
        "        is_primary=True,\n",
        "        auto_id=False,\n",
        "    )\n",
        "\n",
        "    passage = FieldSchema(\n",
        "        name=\"passage\",\n",
        "        dtype=DataType.VARCHAR,\n",
        "        max_length=2600,\n",
        "    )\n",
        "    embedding = FieldSchema(\n",
        "        name=\"embedding\",\n",
        "        dtype=DataType.FLOAT_VECTOR,\n",
        "        dim=embed_dim,\n",
        "    )\n",
        "\n",
        "    schema = CollectionSchema(\n",
        "        fields=[id_, passage, embedding],\n",
        "        description=\"RAG Wikipedia passages\",\n",
        "        auto_id=False,\n",
        "    )\n",
        "\n",
        "    return schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j0B_-nCeHJA9",
      "metadata": {
        "id": "j0B_-nCeHJA9"
      },
      "outputs": [],
      "source": [
        "def setup_collection(embed_dim, collection_name, rag_data):\n",
        "    if collection_name in client.list_collections():\n",
        "        client.drop_collection(collection_name)\n",
        "\n",
        "    schema = create_schema(embed_dim)\n",
        "\n",
        "    client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        schema=schema,\n",
        "    )\n",
        "\n",
        "    client.insert(collection_name=collection_name, data=rag_data)\n",
        "\n",
        "    index_params = MilvusClient.prepare_index_params()\n",
        "    index_params.add_index(\n",
        "        field_name=\"embedding\",\n",
        "        index_type=\"FLAT\",\n",
        "        metric_type=\"COSINE\",\n",
        "    )\n",
        "    client.create_index(\n",
        "        collection_name=collection_name,\n",
        "        index_params=index_params,\n",
        "    )\n",
        "\n",
        "    client.load_collection(collection_name=collection_name)\n",
        "\n",
        "    print(f\"{collection_name} created and loaded into memory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-eXXuxtqDPWE",
      "metadata": {
        "id": "-eXXuxtqDPWE"
      },
      "outputs": [],
      "source": [
        "def generate_answer(prompt, model, tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return answer[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ocyt0Etl-hdp",
      "metadata": {
        "id": "Ocyt0Etl-hdp"
      },
      "outputs": [],
      "source": [
        "get_output_file_name = lambda exp_name: f\"{results_dir}/out_{exp_name}.json\"\n",
        "get_results_file_name = lambda exp_name: f\"{results_dir}/exp_{exp_name}.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6wnk6HQzG-",
      "metadata": {
        "id": "4c6wnk6HQzG-"
      },
      "outputs": [],
      "source": [
        "def save_outputs(results, exp_name):\n",
        "    with open(get_output_file_name(exp_name), \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    print(f\"Saved {results_dir}/out_{exp_name}.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7_FP94-FDPTL",
      "metadata": {
        "id": "7_FP94-FDPTL"
      },
      "outputs": [],
      "source": [
        "def save_results(config, metrics, exp_name):\n",
        "    output = {\n",
        "        \"config\": config,\n",
        "        \"metrics\": metrics,\n",
        "    }\n",
        "\n",
        "    filename = get_results_file_name(exp_name)\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(output, f)\n",
        "\n",
        "    print(f\"Saved {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ndtpi_-Ps4yy",
      "metadata": {
        "id": "Ndtpi_-Ps4yy"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def select_random_subset(results, size=25):\n",
        "    random_indices = np.random.choice(len(results), size=size, replace=False)\n",
        "    return [results[i] for i in random_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJPC6mHvRLtW",
      "metadata": {
        "collapsed": true,
        "id": "EJPC6mHvRLtW"
      },
      "outputs": [],
      "source": [
        "squad_metric = load(\"squad\")\n",
        "\n",
        "\n",
        "def perform_basic_evaluation(results):\n",
        "    predictions = [\n",
        "        {\n",
        "            \"id\": str(i),\n",
        "            \"prediction_text\": r[\"predicted_answer\"],\n",
        "        }\n",
        "        for i, r in enumerate(results)\n",
        "    ]\n",
        "\n",
        "    references = [\n",
        "        {\n",
        "            \"id\": str(i),\n",
        "            \"answers\": {\n",
        "                \"text\": [r[\"ground_truth\"]],\n",
        "                \"answer_start\": [0],\n",
        "            },\n",
        "        }\n",
        "        for i, r in enumerate(results)\n",
        "    ]\n",
        "\n",
        "    metrics = squad_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {\n",
        "        \"f1_score\": metrics[\"f1\"],\n",
        "        \"exact_match\": metrics[\"exact_match\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q093f-ApSTNL",
      "metadata": {
        "id": "Q093f-ApSTNL"
      },
      "outputs": [],
      "source": [
        "def perform_ragas_evaluation(results):\n",
        "    results = select_random_subset(results, size=100)\n",
        "\n",
        "    data = {\n",
        "        \"question\": [r[\"question\"] for r in results],\n",
        "        \"answer\": [r[\"predicted_answer\"] for r in results],\n",
        "        \"contexts\": [r[\"contexts\"] for r in results],\n",
        "        \"ground_truth\": [r[\"ground_truth\"] for r in results],\n",
        "    }\n",
        "\n",
        "    dataset = Dataset.from_dict(data)\n",
        "\n",
        "    config = RunConfig(max_workers=8, timeout=60)\n",
        "    eval_result_sequential = evaluate(\n",
        "        dataset, metrics=[answer_relevancy], run_config=config\n",
        "    )\n",
        "\n",
        "    eval_result_parallel = evaluate(\n",
        "        dataset,\n",
        "        metrics=[faithfulness, context_recall, context_precision],\n",
        "        run_config=config,\n",
        "    )\n",
        "\n",
        "    # Combine results\n",
        "    agg_scores = {}\n",
        "\n",
        "    for metric, values in eval_result_parallel._scores_dict.items():\n",
        "        agg_scores[metric] = float(np.nanmean(values))\n",
        "\n",
        "    for metric, values in eval_result_sequential._scores_dict.items():\n",
        "        agg_scores[metric] = float(np.nanmean(values))\n",
        "\n",
        "    return agg_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OsTnRYE0Q-7A",
      "metadata": {
        "id": "OsTnRYE0Q-7A"
      },
      "outputs": [],
      "source": [
        "def evaluate_results(results):\n",
        "    return {**perform_basic_evaluation(results), **perform_ragas_evaluation(results)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjG9QFrEJJ2S",
      "metadata": {
        "id": "wjG9QFrEJJ2S"
      },
      "source": [
        "# Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zAiYWJ6MDPQk",
      "metadata": {
        "id": "zAiYWJ6MDPQk"
      },
      "outputs": [],
      "source": [
        "prompts = configs[\"prompts\"]\n",
        "embedding_models = configs[\"embedding_models\"]\n",
        "top_ks = configs[\"top_k\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JGyEcWV0DPMq",
      "metadata": {
        "id": "JGyEcWV0DPMq"
      },
      "outputs": [],
      "source": [
        "client = MilvusClient(\"rag_wikipedia_mini.db\")\n",
        "\n",
        "for embed_model_name, embed_dim, embedding_model in tqdm(embedding_models):\n",
        "    embeddings, rag_data = create_embeddings_and_rag_data(embedding_model)\n",
        "    setup_collection(embed_dim, embed_model_name, rag_data)\n",
        "\n",
        "    for prompt_name, prompt_generator in prompts:\n",
        "        for top_k in top_ks:\n",
        "            print(\"=\" * 18)\n",
        "            print(\n",
        "                f\"Embedding: {embed_model_name}, Prompt: {prompt_name}, Top K: {top_k}\"\n",
        "            )\n",
        "\n",
        "            experiment_name = f\"{embed_model_name}_{prompt_name}_{top_k}\"\n",
        "            results = []\n",
        "\n",
        "            if os.path.exists(get_output_file_name(experiment_name)):\n",
        "                print(f\"Output for {experiment_name} exists! Skipping...\")\n",
        "            else:\n",
        "                for index, row in tqdm(queries.iterrows(), total=len(queries)):\n",
        "                    query = row[\"question\"]\n",
        "\n",
        "                    search_results = client.search(\n",
        "                        collection_name=embed_model_name,\n",
        "                        data=[embedding_model.encode(query).tolist()],\n",
        "                        output_fields=[\"passage\"],\n",
        "                        limit=top_k,\n",
        "                    )\n",
        "\n",
        "                    context = [result[\"passage\"] for result in search_results[0]]\n",
        "                    context_str = \"\\n\".join(context)\n",
        "\n",
        "                    prompt = prompt_generator(query, context)\n",
        "                    answer = generate_answer(prompt, model, tokenizer)\n",
        "                    results.append(\n",
        "                        {\n",
        "                            \"question\": query,\n",
        "                            \"predicted_answer\": answer,\n",
        "                            \"ground_truth\": row[\"answer\"],\n",
        "                            \"contexts\": context,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                save_outputs(results, experiment_name)\n",
        "\n",
        "            if os.path.exists(get_results_file_name(experiment_name)):\n",
        "                print(f\"Results for {experiment_name} exists! Skipping...\")\n",
        "            else:\n",
        "                if not results:\n",
        "                    with open(get_output_file_name(experiment_name), \"r\") as f:\n",
        "                        results = json.load(f)\n",
        "                metrics = evaluate_results(results)\n",
        "                save_results(\n",
        "                    {\n",
        "                        \"embedding_model\": embed_model_name,\n",
        "                        \"prompt\": prompt_name,\n",
        "                        \"top_k\": top_k,\n",
        "                    },\n",
        "                    metrics,\n",
        "                    experiment_name,\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C8m_NCxs-xPo",
      "metadata": {
        "id": "C8m_NCxs-xPo"
      },
      "outputs": [],
      "source": [
        "!zip -r results.zip results/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
